# -*- coding: utf-8 -*-
"""DTS-Hanif Al Irsyad

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qzcQXj_Is5KLGLAxJ2eWaI_icPy9l_Nk

**M-07 Pengembang Machine Learning dan Front-End**

Hanif Al Irsyad

Universitas Amikom Yogyakarta

Sleman,Yogyakarta
"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM, Bidirectional, Dropout
import matplotlib.pyplot as plt
import tensorflow as tf

df = pd.read_csv('/content/coffee.csv', encoding= 'unicode_escape')
df.head()

df.info()

df['Date']=pd.to_datetime(df['Date'])
df['Date'].head()
df['Open'].fillna(df['Open'].mean(), inplace=True) 
df = df[['Date','Open' ]]
df.head(10)

coffee=df[['Date','Open']].copy()
coffee['Time'] = coffee['Date'].dt.date
df_new=coffee.drop('Date',axis=1)
df_new.set_index('Time', inplace= True)
df_new.head(10)

df_new.info()

df_new.dropna()

x = df['Date'].values
y = df['Open'].values

plt.figure(figsize=(20,6))
plt.plot(df_new)
plt.title('Coffee Open Price')
plt.xlabel('Date')
plt.ylabel('Price in USD')
plt.show()

from sklearn.preprocessing import MinMaxScaler

data = df['Open'].values
data_new = data.reshape(-1,1)
scaler = MinMaxScaler()
data_scaler = scaler.fit_transform(data_new)
data_scaler = data_scaler.reshape(1,-1)
data_scaler = np.hstack(data_scaler)
data_scaler

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

def model_forecast(model, series, window_size):
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size))
    ds = ds.batch(32).prefetch(1)
    forecast = model.predict(ds)
    return forecast

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, data_scaler, test_size=0.2, random_state=0, shuffle=False)
print(len(X_train), len(X_test))

set_latih = windowed_dataset(y_train, 
                             window_size=64, 
                             batch_size=200, 
                             shuffle_buffer=1000,
                             )

val_test = windowed_dataset(y_test, 
                            window_size=64, 
                            batch_size=200, 
                            shuffle_buffer=1000)

model = tf.keras.models.Sequential([
    Bidirectional(LSTM(60, return_sequences=True)),
    Bidirectional(LSTM(60)),
    Dense(30, activation="relu"),
    Dense(10, activation="relu"),
    Dense(1),
])

data_mae = (np.max(data_scaler) - np.min(data_scaler)) * 0.1
print(data_mae)

class Callbackdts(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')< data_mae) and (logs.get('val_mae') < data_mae):
      self.model.stop_training = True
      print('\nFor Epoch', epoch, ' training has been stopped.''\n Because MAE of the model has reach < 10% of data scale')
callbacks = Callbackdts()

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

history = model.fit(set_latih,
                    epochs=100,
                    validation_data=val_test, 
                    callbacks=[callbacks])

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Akurasi Model')
plt.ylabel('Mae')
plt.xlabel('epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()